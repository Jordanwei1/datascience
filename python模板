[python] view plain copy
# 通用的预处理框架  
  
import pandas as pd  
import numpy as np  
import scipy as sp  
  
# 文件读取  
def read_csv_file(f, logging=False):  
    print("==========读取数据=========")  
    data =  pd.read_csv(f)  
    if logging:  
        print(data.head(5))  
        print(f, "包含以下列")  
        print(data.columns.values)  
        print(data.describe())  
        print(data.info())  
    return data  
 
 
 
# 通用的LogisticRegression框架  
  
import pandas as pd  
import numpy as np  
from scipy import sparse  
from sklearn.preprocessing import OneHotEncoder  
from sklearn.linear_model import LogisticRegression  
from sklearn.preprocessing import StandardScaler  
  
# 1. load data  
df_train = pd.DataFrame()  
df_test  = pd.DataFrame()  
y_train = df_train['label'].values  
  
# 2. process data  
ss = StandardScaler()  
  
  
# 3. feature engineering/encoding  
# 3.1 For Labeled Feature  
enc = OneHotEncoder()  
feats = ["creativeID", "adID", "campaignID"]  
for i, feat in enumerate(feats):  
    x_train = enc.fit_transform(df_train[feat].values.reshape(-1, 1))  
    x_test = enc.fit_transform(df_test[feat].values.reshape(-1, 1))  
    if i == 0:  
        X_train, X_test = x_train, x_test  
    else:  
        X_train, X_test = sparse.hstack((X_train, x_train)), sparse.hstack((X_test, x_test))  
  
# 3.2 For Numerical Feature  
# It must be a 2-D Data for StandardScalar, otherwise reshape(-1, len(feats)) is required  
feats = ["price", "age"]  
x_train = ss.fit_transform(df_train[feats].values)  
x_test  = ss.fit_transform(df_test[feats].values)  
X_train, X_test = sparse.hstack((X_train, x_train)), sparse.hstack((X_test, x_test))  
  
# model training  
lr = LogisticRegression()  
lr.fit(X_train, y_train)  
proba_test = lr.predict_proba(X_test)[:, 1]  



 LightGBM
1. 二分类
import lightgbm as lgb
import pandas as pd
import numpy as np
import pickle
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split
 
print("Loading Data ... ")
 
# 导入数据
train_x, train_y, test_x = load_data()
 
# 用sklearn.cross_validation进行训练数据集划分，这里训练集和交叉验证集比例为7：3，可以自己根据需要设置
X, val_X, y, val_y = train_test_split(
    train_x,
    train_y,
    test_size=0.05,
    random_state=1,
    stratify=train_y ## 这里保证分割后y的比例分布与原数据一致
)
 
X_train = X
y_train = y
X_test = val_X
y_test = val_y
 
 
# create dataset for lightgbm
lgb_train = lgb.Dataset(X_train, y_train)
lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)
# specify your configurations as a dict
params = {
    'boosting_type': 'gbdt',
    'objective': 'binary',
    'metric': {'binary_logloss', 'auc'},
    'num_leaves': 5,
    'max_depth': 6,
    'min_data_in_leaf': 450,
    'learning_rate': 0.1,
    'feature_fraction': 0.9,
    'bagging_fraction': 0.95,
    'bagging_freq': 5,
    'lambda_l1': 1,  
    'lambda_l2': 0.001,  # 越小l2正则程度越高
    'min_gain_to_split': 0.2,
    'verbose': 5,
    'is_unbalance': True
}
 
# train
print('Start training...')
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10000,
                valid_sets=lgb_eval,
                early_stopping_rounds=500)
 
print('Start predicting...')
 
preds = gbm.predict(test_x, num_iteration=gbm.best_iteration)  # 输出的是概率结果
 
# 导出结果
threshold = 0.5
for pred in preds:
    result = 1 if pred > threshold else 0
 
# 导出特征重要性
importance = gbm.feature_importance()
names = gbm.feature_name()
with open('./feature_importance.txt', 'w+') as file:
    for index, im in enumerate(importance):
        string = names[index] + ', ' + str(im) + '\n'
        file.write(string)
        


2. 多分类
import lightgbm as lgb
import pandas as pd
import numpy as np
import pickle
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split
 
print("Loading Data ... ")
 
# 导入数据
train_x, train_y, test_x = load_data()
 
# 用sklearn.cross_validation进行训练数据集划分，这里训练集和交叉验证集比例为7：3，可以自己根据需要设置
X, val_X, y, val_y = train_test_split(
    train_x,
    train_y,
    test_size=0.05,
    random_state=1,
    stratify=train_y ## 这里保证分割后y的比例分布与原数据一致
)
 
X_train = X
y_train = y
X_test = val_X
y_test = val_y
 
 
# create dataset for lightgbm
lgb_train = lgb.Dataset(X_train, y_train)
lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)
# specify your configurations as a dict
params = {
    'boosting_type': 'gbdt',
    'objective': 'multiclass',
    'num_class': 9,
    'metric': 'multi_error',
    'num_leaves': 300,
    'min_data_in_leaf': 100,
    'learning_rate': 0.01,
    'feature_fraction': 0.8,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'lambda_l1': 0.4,
    'lambda_l2': 0.5,
    'min_gain_to_split': 0.2,
    'verbose': 5,
    'is_unbalance': True
}
 
# train
print('Start training...')
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=10000,
                valid_sets=lgb_eval,
                early_stopping_rounds=500)
 
print('Start predicting...')
 
preds = gbm.predict(test_x, num_iteration=gbm.best_iteration)  # 输出的是概率结果
 
# 导出结果
for pred in preds:
    result = prediction = int(np.argmax(pred))
 
# 导出特征重要性
importance = gbm.feature_importance()
names = gbm.feature_name()
with open('./feature_importance.txt', 'w+') as file:
    for index, im in enumerate(importance):
        string = names[index] + ', ' + str(im) + '\n'
        file.write(string)
        
XGBoost
1. 二分类
import numpy as np
import pandas as pd
import xgboost as xgb
import time
from sklearn.model_selection import StratifiedKFold
 
 
from sklearn.model_selection import train_test_split
train_x, train_y, test_x = load_data()
 
# 构建特征
 
 
# 用sklearn.cross_validation进行训练数据集划分，这里训练集和交叉验证集比例为7：3，可以自己根据需要设置
X, val_X, y, val_y = train_test_split(
    train_x,
    train_y,
    test_size=0.01,
    random_state=1,
    stratify=train_y
)
 
# xgb矩阵赋值
xgb_val = xgb.DMatrix(val_X, label=val_y)
xgb_train = xgb.DMatrix(X, label=y)
xgb_test = xgb.DMatrix(test_x)
 
# xgboost模型 #####################
 
params = {
    'booster': 'gbtree',
    # 'objective': 'multi:softmax',  # 多分类的问题、
    # 'objective': 'multi:softprob',   # 多分类概率
    'objective': 'binary:logistic',
    'eval_metric': 'logloss',
    # 'num_class': 9,  # 类别数，与 multisoftmax 并用
    'gamma': 0.1,  # 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。
    'max_depth': 8,  # 构建树的深度，越大越容易过拟合
    'alpha': 0,   # L1正则化系数
    'lambda': 10,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。
    'subsample': 0.7,  # 随机采样训练样本
    'colsample_bytree': 0.5,  # 生成树时进行的列采样
    'min_child_weight': 3,
    # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言
    # ，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。
    # 这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。
    'silent': 0,  # 设置成1则没有运行信息输出，最好是设置为0.
    'eta': 0.03,  # 如同学习率
    'seed': 1000,
    'nthread': -1,  # cpu 线程数
    'missing': 1,
    'scale_pos_weight': (np.sum(y==0)/np.sum(y==1))  # 用来处理正负样本不均衡的问题,通常取：sum(negative cases) / sum(positive cases)
    # 'eval_metric': 'auc'
}
plst = list(params.items())
num_rounds = 2000  # 迭代次数
watchlist = [(xgb_train, 'train'), (xgb_val, 'val')]
 
# 交叉验证
result = xgb.cv(plst, xgb_train, num_boost_round=200, nfold=4, early_stopping_rounds=200, verbose_eval=True, folds=StratifiedKFold(n_splits=4).split(X, y))
 
# 训练模型并保存
# early_stopping_rounds 当设置的迭代次数较大时，early_stopping_rounds 可在一定的迭代次数内准确率没有提升就停止训练
model = xgb.train(plst, xgb_train, num_rounds, watchlist, early_stopping_rounds=200)
model.save_model('../data/model/xgb.model')  # 用于存储训练出的模型
 
preds = model.predict(xgb_test)
 
# 导出结果
threshold = 0.5
for pred in preds:
    result = 1 if pred > threshold else 0
    
Keras
1. 二分类
import numpy as np
import pandas as pd
import time
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
 
from keras.models import Sequential
from keras.layers import Dropout
from keras.layers import Dense, Activation
from keras.utils.np_utils import to_categorical
 
# coding=utf-8
from model.util import load_data as load_data_1
from model.util_combine_train_test import load_data as load_data_2
from sklearn.preprocessing import StandardScaler # 用于特征的标准化
from sklearn.preprocessing import Imputer
 
print("Loading Data ... ")
# 导入数据
train_x, train_y, test_x = load_data()
 
# 构建特征
X_train = train_x.values
X_test  = test_x.values
y = train_y
 
imp = Imputer(missing_values='NaN', strategy='mean', axis=0)
X_train = imp.fit_transform(X_train)
 
sc = StandardScaler()
sc.fit(X_train)
X_train = sc.transform(X_train)
X_test  = sc.transform(X_test)
 
 
model = Sequential()
model.add(Dense(256, input_shape=(X_train.shape[1],)))
model.add(Activation('tanh'))
model.add(Dropout(0.3))
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.3))
model.add(Dense(512))
model.add(Activation('tanh'))
model.add(Dropout(0.3))
model.add(Dense(256))
model.add(Activation('linear'))
model.add(Dense(1)) # 这里需要和输出的维度一致
model.add(Activation('sigmoid'))
 
# For a multi-class classification problem
model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])
 
epochs = 100
model.fit(X_train, y, epochs=epochs, batch_size=2000, validation_split=0.1, shuffle=True)
 
# 导出结果
threshold = 0.5
for index, case in enumerate(X_test):
    case =np.array([case])
    prediction_prob = model.predict(case)
    prediction = 1 if prediction_prob[0][0] > threshold else 0
    
2. 多分类
import numpy as np
import pandas as pd
import time
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
 
from keras.models import Sequential
from keras.layers import Dropout
from keras.layers import Dense, Activation
from keras.utils.np_utils import to_categorical
 
# coding=utf-8
from model.util import load_data as load_data_1
from model.util_combine_train_test import load_data as load_data_2
from sklearn.preprocessing import StandardScaler # 用于特征的标准化
from sklearn.preprocessing import Imputer
 
print("Loading Data ... ")
# 导入数据
train_x, train_y, test_x = load_data()
 
# 构建特征
X_train = train_x.values
X_test  = test_x.values
y = train_y
 
# 特征处理
sc = StandardScaler()
sc.fit(X_train)
X_train = sc.transform(X_train)
X_test  = sc.transform(X_test)
y = to_categorical(y) ## 这一步很重要，一定要将多类别的标签进行one-hot编码
 
 
model = Sequential()
model.add(Dense(256, input_shape=(X_train.shape[1],)))
model.add(Activation('tanh'))
model.add(Dropout(0.3))
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.3))
model.add(Dense(512))
model.add(Activation('tanh'))
model.add(Dropout(0.3))
model.add(Dense(256))
model.add(Activation('linear'))
model.add(Dense(9)) # 这里需要和输出的维度一致
model.add(Activation('softmax'))
 
# For a multi-class classification problem
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
 
epochs = 200
model.fit(X_train, y, epochs=epochs, batch_size=200, validation_split=0.1, shuffle=True)
 
# 导出结果
for index, case in enumerate(X_test):
    case = np.array([case])
    prediction_prob = model.predict(case)
    prediction = np.argmax(prediction_prob)
    
    
    
    
    
    
处理正负样本不均匀的案例
有些案例中，正负样本数量相差非常大，数据严重unbalanced，这里提供几个解决的思路
# 计算正负样本比例
positive_num = df_train[df_train['label']==1].values.shape[0]
negative_num = df_train[df_train['label']==0].values.shape[0]
print(float(positive_num)/float(negative_num))

主要思路
1. 手动调整正负样本比例
2. 过采样 Over-Sampling
对训练集里面样本数量较少的类别（少数类）进行过采样，合成新的样本来缓解类不平衡，比如SMOTE算法
3. 欠采样 Under-Sampling
4. 将样本按比例一一组合进行训练，训练出多个弱分类器，最后进行集成



排序算法
def quick_sort(a,l,r):
	if(l>=r):
		return 
	key = a[l]
	i,j = l-1,r+1
	while(i<j):
		i+=1
		j-=1
		while(a[j]>key):
			j-=1
		while(a[i]<key):
			i+=1
		if(i<j):
			temp = a[i]
			a[i] = a[j]
			a[j] = temp
	quick_sort(a,l,j)
	quick_sort(a,j+1,r)
  
  
  归并排序
  def merge_sort(a,temp,l,r):
	if(l>=r):
		return
	mid = (r+l)//2
	merge_sort(a,temp,l,mid)
	merge_sprt(a,temp,mid+1,r)
	
	k = 0
	while(i<=mid and j<=r):
		if(a[i]<a[j]):
			temp[k] = a[i]
			i+=1
		else:
			temp[k] = a[j]
			j+=1
		k+=1
		
	while(i<=mid):
		temp[k] = a[i]
		k+=1
		i+=1
	while(j<=r):
		temp[k] = a[j]
		j+=1
		k+=1
	
	k = 0
	i = l
	while(i<=r):
		a[i] = temp[k]
		k+=1
		i+=1



KMP算法
def getNe(s):
	"""
	求取next数组
	"""
	m = len(s)
	#数组取大一点是没有关系的
	next = [0 for _ in range(m+10)]
	i = 1
	j = 0
	while(i<m):
		while(j>0 and s[i]!=s[j]):
			j = next[j-1]
		if(s[i]==s[j]);
			j+=1
		next[i] = j
		i+=1
	return next


def KMP(s1,s2):
	"""
	返回第x个字符之后会匹配上
	"""
	next = getNe(s2)
	m_s1 = len(s1)
	m_s2 = len(s2)
	
	i = 0
	j = 0
	while(i<m_s1):
		while(j>0 and s1[i]!=s2[j]):
			j = next[j-1]
		if(s1[i]==s2[j]):
			j+=1
		if(j==m_s2):
			return i - m_s2 + 1
		i+=1
	return -1


Floyd 算法
def Floyd(g):

    for k in range(1,100):
        for i in range(1,100):
            for j in range(1,100):
                g[i][j] = min(g[i][k]+g[k][j],g[i][j])
                
                
Dijkstra 算法
def Dijkstra(g):
	st = [False for _ in range(n+1)]
	dist = [float("inf") for _ in range(n+1)]
	dist[1] = 0
	for i in range(n-1):
		t = -1
		for j in range(1,n+1):
			if(not st[j] and (t==-1 or dist[t]>dist[j]):
				t = j 
		
		for j in range(1,n+1);
			dist[j] = min(dist[j],dist[t]+g[t][j])
	if(dist[n]==float("inf")):
		return False



BellMan-Ford 算法
class node:
	a = None
	b = None
	c = None
Nodes = [node(a,b,c) for _ in range(m)]

def BellmanFord():
	dist = [float("inf") for _ in range(1,n+1)]
	dist[i] = 0
	for i in range(n):
		for j in range(m);
			a,b,w = Nodes[j].a,Nodes[j].b,Nodes[j].c
			dist[b] = min(dist[b],dist[a]+w)


SPFA 算法

def SPFA():
	import collections
	q = collections.deque()
	dist = [float("inf") for _ in range(n)]
	st = [False for _ in range(n)]

	q.append(1)
	st[1]=True
	while(len(q)):
		t = q.pop()
		st[t] = False
		i = h[t]
		while(i!=-1):
			j = e[i]
			if(dist[j]>dist[t]+w[i]):
				dist[j] = dist[t]+w[i]
				if(not st[j]):
					q.append(j)
					st[j] = True
			i = ne[i]
	# 这个自己取一个比较大的数
 	if(dist[n]>=100000):
 		return False


Prim 算法

def Prime(g):
    """
    这里的话还是使用邻接矩阵的
    :return:
    """
    dist = [float("inf") for _ in range(100)]
    st = [False for _ in range(100)]
    res = 0
    for i in range(100):

        t = -1
        for j in range(1,101):
            if((t == -1 and not st[j]) or dist[t]>dist[j]):
                t = j

        if(i and dist[t] == float("inf")):
            return float("inf")

        if(i):
            res+=dist[t]

        for j in range(100):
            dist[j] = min(dist[j],g[t][j])


Kruskra 算法

def Kruskra():
    """
    定义这个数据是存储边的
    :return:
    """
    # 这个P的话就是Parents数组
    p = [0]
    res = 0
    cnt = 0
    Nodes = [Node() for _ in range(100)]
    sorted(Nodes,lambda node:node.c)
    for i in range(100):
        node = Nodes[i]
        a = find(node.a,p)
        b = find(node.b,p)
        if(a!=b):
            p[a] = b
            res+=node.c
            cnt+=1
        #此时是不连通的
        if(cnt<100-1):
            return float("inf")


染色法

"""
0 表示没有染色，1表示白色，2表示黑色
"""

def upColor(color,a,c):
	color[a] = c
	i = h[a]
	while(i!=-1):
		j = e[i]
		if(not color[j]):
			if(not upColor(st,j,3-c)):
				return False
		elif(color[j]==c):
			return False
		i = ne[i]
	return True

def check():
	
	for i in range(1,n+1):
		if(not upColor(st,i,1)):
			return False
	return True
  
  
  
  Hunger算法
  def Match(st,match,a):
	
	i = h[a]
    while(i!=-1):
        j = e[i]
        if(not st[j]):
            st[j] = True
            if(match[j]==0 or Match(st,match,j)):
                match[j] = a
                return True
        i = ne[i]
	return False

def Hunager():

	res = 0 #匹配个数
	Match = [0 for _ in range(10000)] #右侧的集合元素和左侧的谁进行了匹配
	for i in range(1,n1+1):
		# 表示右边的那个集合元素有没有匹配到
		st = [False for _ in range(10000)]
		if(Match(st,match,i)):
			res+=1
	return res
		
	一、二分模板
题目地址

题目：给你一个 m * n 的矩阵 grid，矩阵中的元素无论是按行还是按列，都以非递增顺序排列。 

请你统计并返回 grid 中 负数 的数目。

思路：找到每排的第一个负数，后面就都是负数了，最后累计负数数目

模板：
def binary_search(nums, target):
    low = 0
    high = len(nums) - 1
    while low <= high:
        mid = (low + high) // 2
 
        if nums[mid] == target:
            return mid
        if nums[mid] > target:
            high = mid - 1
        else:
            low = mid + 1
    return low
 
 
a = [1, 3, 7, 9, 14, 20, 24]
print(binary_search(a, 0))  # 0
print(binary_search(a, 1))  # 0
print(binary_search(a, 2))  # 1
print(binary_search(a, 4))  # 2
print(binary_search(a, 8))  # 3
 
print(binary_search(a, 11))  # 4
print(binary_search(a, 16))  # 5
print(binary_search(a, 24))  # 6
print(binary_search(a, 29))  # 7



from typing import List
 
 
class Solution:
    def countNegatives(self, grid: List[List[int]]) -> int:
        res = 0
        m = grid.__len__()
        n = grid[0].__len__()
        for i in range(m):
            # 二分找到0的位置
            low, high = 0, n - 1
            while low < high:
                mid = (low + high) // 2
                if grid[i][mid] >= 0:
                    low = mid + 1
                else:
                    high = mid
 
            if grid[i][low] < 0:
                res += n - low
        return res
 
 
s = Solution()
print(s.countNegatives([[4, 3, 2, -1], [3, 2, 1, -1], [1, 1, -1, -2], [-1, -1, -2, -3]]))
# 8

二、递归模板
题目地址

题目：

在经典汉诺塔问题中，有 3 根柱子及 N 个不同大小的穿孔圆盘，盘子可以滑入任意一根柱子。一开始，所有盘子自上而下按升序依次套在第一根柱子上(即每一个盘子只能放在更大的盘子上面)。移动圆盘时受到以下限制:
(1) 每次只能移动一个盘子;
(2) 盘子只能从柱子顶端滑出移到下一根柱子;
(3) 盘子只能叠在比它大的盘子上。

请编写程序，用栈将所有盘子从第一根柱子移到最后一根柱子。

你需要原地修改栈。

思路：

将N个盘子分为N，N-1两部分，将N-1个盘子看成一个整体，分三步完成移动即可完成

1. N-1部分由A->B

2. N部分由A->C

3. N-1部分由B->C

再把上述N-1部分分为N-1和N-2两部分，继续重复的分三步完成移动

模板：搞清楚递归退出条件是什么，问题怎么分解递归解决的
def fib(n):
    # 递归退出条件
    if n < 2:
        return n
    else:
        # 分解递归
        return fib(n-1) + fib(n-2)
 
print(fib(10))


class Solution:
    def hanota(self, A: List[int], B: List[int], C: List[int]) -> None:
        def hanoi(n,a,b,c):
            # 递归退出条件
            if n == 1:
                c.append(a.pop())
            else:
                # 分解递归
                hanoi(n-1,a,c,b)
                c.append(a.pop())
                hanoi(n-1,b,a,c)
 
        hanoi(len(A),A,B,C)
        
        
二叉树遍历模板
# 前序优先遍历
def qianxu(p: TreeNode):
    res = []
    stack = []
    while (stack.__len__() != 0 or p):
        if p:
            res.append(p.val)
            stack.append(p)
            p = p.left
        else:
            p = stack.pop(-1)
            p = p.right
    return res
 
 
# 中序优先遍历
def zhongxu(p: TreeNode):
    res = []
    stack = []
    while (stack.__len__() != 0 or p):
        if p:
            stack.append(p)
            p = p.left
        else:
            p = stack.pop(-1)
            res.append(p.val)
            p = p.right
    return res
 
 
# 后序优先遍历
def houxu(p: TreeNode):
    res = []
    stack = []
    d = dict()
    while (p or stack.__len__() != 0):
        if p:
            stack.append(p)
            d.update({p.val: 1})
            p = p.left
        else:
            p = stack[-1]
            if d[p.val] == 2:
                stack.pop(-1)
                res.append(p.val)
                p = None
            else:
                d[p.val] = 2
                p = p.right
 
    return res
 
def houxu2( root):
    '''
    利用两个栈实现
    '''
    s1 = []
    s2 = []
    s1.append( root )
    while s1:
        node = s1.pop()
        s2.append( node )
        if node.lchild:
            s1.append( node.lchild )
        if node.rchild:
            s1.append( node.rchild )
    while s2:
        print(s2.pop().value)
 
# 层次遍历，用队列
def cengci(p: TreeNode):
    res = []
    queue = [p]
    while (queue.__len__() != 0):
        p = queue.pop(0)
        res.append(p.val)
        if p.left:
            queue.append(p.left)
        if p.right:
            queue.append(p.right)
    return res
 
# 递归
# 先序
def preorder(root):
    if not root:
        return 
    print(root.val)
    preorder(root.left)
    preorder(root.right) 
# 中序
def inorder(root):
    if not root:
        return 
    inorder(root.left)
    print(root.val)
    inorder(root.right)
# 后序
def postorder(root):
    if not root:
        return 
    postorder(root.left)
    postorder(root.right)
    print(root.val)
    
    
    def exists(self, idx: int, d: int, node: TreeNode) -> bool:
        """
        Last level nodes are enumerated from 0 to 2**d - 1 (left -> right).
        Return True if last level node idx exists. 
        Binary search with O(d) complexity.
        """
        left, right = 0, 2**d - 1
        for _ in range(d):
            pivot = (left + right) // 2
            if idx <= pivot:
                node = node.left
                right = pivot
            else:
                node = node.right
                left = pivot + 1
        return node is not None

递归--代码模板
def recursion(level, param1, param2, ...): 
    # recursion terminator 
    if level > MAX_LEVEL: 
    process_result 
    return 
    # process logic in current level 
    process(level, data...) 
    # drill down 
    self.recursion(level + 1, p1, ...) 
    # reverse the current level status if needed
    
    
   
